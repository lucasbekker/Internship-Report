\chapter{Theoretical parallel speedup}

\hspace{4mm} Applying parallel programming techniques to accelerate an algorithm is a very common task. This section will discuss some of the available theory for making predictions about the maximum speedup that can be achieved.

\section{Amdahl's law}

\hspace{4mm} Amdahl's law relates the maximum achievable speedup of a task to the amount of time that the task executes elements that can be parallelized.\autocite[]{wiki_amdahl} This is based on two assumptions:

\begin{itemize}
    \item The task can be divided in a serial part and a parallel part.
    \item The parallel part scales linearly with the amount of available parallel nodes.
\end{itemize}

\begin{equation} \label{Amd 1 eq.}
    \begin{split}
        \text{T}_{total} = & \ \text{T}_{serial} + \text{T}_{parallel}\\
        \text{Speedup}(k) = & \ \frac{\text{T}_{total}}{\text{T}_{serial} + (\frac{\text{T}_{parallel}}{k})}\\
        \\
        k := & \ \text{number of parallel nodes}
    \end{split}
\end{equation}

\vspace{2mm}

As an example, take a task that consist of $95 \%$ parallelizable work. The maximum speedup can never exceed $20$, no matter how much parallel nodes are employed.

\subsection{Parallel slowdown}

\hspace{4mm} Parallel slowdown is an effect that can plague certain parallel tasks.\autocite[]{wiki_parsl} Scaling the task beyond a certain amount of parallel nodes causes a runtime increase instead of the expected decrease. This is typically associated with a communications bottleneck, where the time required for communication between the parallelly executed parts grows faster than can be compensated by dividing the workload among the parallel nodes.

\newpage

\section{Compensated Amdahl's law}

\hspace{4mm}Amdahl's law assumes that dividing the parallel part of the task among many nodes doesn't inflict any sort of runtime penalty. That assumption is certainly reasonable for embarrassingly parallel workloads, but is not always correct outside of the embarrassingly parallel case.\\

\noindent Amdahl's law can be compensated for this runtime penalty by specifying and additional part of the task, called $\text{T}_{overhead}(k)$, which is modelled as a function of the amount of nodes:

\begin{equation} \label{Amd 2 eq.}
    \begin{split}
        \text{T}_{total} = & \ \text{T}_{serial} + \text{T}_{parallel}\\
        \text{Speedup}(k) = & \ \frac{\text{T}_{total}}{\text{T}_{serial} + (\frac{\text{T}_{parallel}}{k}) + \text{T}_{overhead}(k)}\\
        \\
        k := & \ \text{number of parallel nodes}
    \end{split}
\end{equation}

\vspace{2mm}

\noindent The exact relation between $\text{T}_{overhead}(k)$ and the amount of nodes is application specific, but two general modelling assumptions seem reasonable:

\begin{itemize}
    \item There is no overhead when only one node is available.
    \item $\text{T}_{overhead}(k)$ consists of a parallelizable and non-parallelizable part.
\end{itemize}

The first assumption is trivial. The second one states that the tasks constituting the overhead can consist of elements that need to be executed sequentially and elements that can be executed concurrently.

\subsection{Modelling parallel slowdown}

\hspace{4mm} Parallel slowdown can be modelled with the compensated Amdahl's law. When it is indeed caused by a communications bottleneck, it seems reasonable to assume the following things:

\begin{itemize}
    \item The overhead consists exclusively of non-parallelizable tasks.
    \item The overhead scales linearly with the amount of parallel nodes.
\end{itemize}

Communication between multiple nodes is usually dominated by non-parallelizable tasks, because it often involves some form of causality. The choice of scaling is debatable, but less then linear scaling is very unlikely, whereas more than linear scaling is very common.\\

\noindent The predicted speedup would in this case be:

\begin{equation} \label{Amd 3 eq.}
    \begin{split}
        \text{T}_{total} = & \ \text{T}_{serial} + \text{T}_{parallel}\\
        \text{Speedup}(k) = & \ \frac{\text{T}_{total}}{\text{T}_{serial} + (\frac{\text{T}_{parallel}}{k}) + ((k - 1) \times \text{T}_{overhead})}\\
        \\
        k := & \ \text{number of parallel nodes}
    \end{split}
\end{equation}

\newpage

\subsection{Compensated example}

\hspace{4mm} An example will be discussed to illustrate the consequences that the overhead compensation of Amdahl's law can have. Like the previous example, the parallel part of the task is set at $95 \%$. The relation for $\text{T}_{overhead}(k)$ is taken from the communications bottlenecked case, with a constant factor of $1 \%$ of $\text{T}_{total}$.

\begin{figure}[h!]
    \includegraphics[width=\linewidth]{figures/Compensated_amdahl.eps}
    \centering
    \caption{Task: $95 \%$ parallelizable, $5 \%$ serial and $1 \%$ overhead.}
    \label{com. amd. ex.}
\end{figure}

The maximum speedup of about $4.255$ is achieved using $10$ parallel nodes, a far cry from the maximum of $20$ that the uncompensated Amdahl's law predicted. Using $150$ nodes results in a speedup of about $0.65$, which is actually a significant performance decrease.

\newpage

\section{Experimental verification}

\hspace{4mm} Accelerating iterative solvers by dividing the workload among parallel nodes is a common practice. However, there is a general rule of thumb that the amount of parallel nodes should not be to large in these cases, as the communication between the processes quickly becomes excessive.\\

\citeauthor{dist_it_solv} investigated the scaling behaviour of distributed iterative solvers, focusing on the Gauss-Seidel method\autocite[]{dist_it_solv}. They provide experimental data on the scaling behaviour for three kind of sparsity patterns:

\begin{figure}[h!]
    \includegraphics[width=0.6\linewidth]{figures/Amd_sparse.png}
    \centering
    \caption{Density: $(1) = 0.015$, $(2) = 0.015$, $(3) = 0.025$ \\ (Taken from \citeauthor{dist_it_solv})}
    \label{com. amd. sp.}
\end{figure}

\begin{figure}[h!]
    \includegraphics[width=0.5\linewidth]{figures/Amd_scale.png}
    \centering
    \caption{Speedups of distributed iterative solvers.\\ (Taken from \citeauthor{dist_it_solv})}
    \label{com. amd. sc.}
\end{figure}

\noindent Figure \ref{com. amd. sc.} shows three things:

\begin{itemize}
    \item Parallel slowdown occurs for all three matrices when using more then $8$ processors.
    \item Sparsity pattern has little effect on the speedup.
    \item Density has a significant effect on the speedup.
\end{itemize}

Worth mentioning is the questionable speedup at $2$ processors, having a value of about $3$ for all three curves. This is odd because Amdahl's law states that the speedup can never exceed the amount of parallel nodes. It might be explained by some caching effect, where the complete data set doesn't fit in the cache of a single processor, but does fit in the cache of two processors. No mention of this was made by \citeauthor{dist_it_solv}.\\

\subsection{Fitted model results}

\hspace{4mm} To evaluate the performance of the compensated Amdahl's law, a comparison is made with the experimental data of \citeauthor{dist_it_solv}.\\

The model represented by equation \ref{Amd 3 eq.} was selected for the comparison, because it is expected that the parallel slowdown was caused by a communications bottleneck. $\text{T}_{parallel}$ was set to $100 \%$ because the algorithm used by \citeauthor{dist_it_solv} contains no serial part. Some experimenting provided the values of $\text{T}_{overhead}$:

\begin{figure}[h!]
    \includegraphics[width=0.7\linewidth]{figures/Amd_ver.eps}
    \centering
    \caption{Red: $\text{T}_{parallel} = 100 \%$, $\text{T}_{overhead} = 1.4 \%$\\
    Blue: $\text{T}_{parallel} = 100 \%$, $\text{T}_{overhead} = 2.0 \%$}
    \label{com. amd. ver.}
\end{figure}

The results are not too bad. The behaviour in the region between $2$ and $6$ processors is a little different, but speedups exceeding the number of parallel nodes cannot be explained with Amdahl's law. Some unknown effect must have had a significant influence on the results of the experimental data, but it is generally very uncommon to see this kind of behaviour.\\

The value of the maximum speedup and the amount of processors required to achieve said speedup are relatively accurate. The experimental results show a slightly less linear and steeper descend then the model, but this just means that the assumption that the overhead scales linearly with the amount of parallel nodes is not completely true. This reflects the discussion of the model provided at section "Modelling parallel slowdown".\\

The red line models the situation of matrix $(3)$, with a density of $0.025$. The blue line models the two other cases with the lower density of $0.015$. The higher density situation has a lower overhead, $1.4 \%$ versus $2.0 \%$. This appears reasonable, because the higher density results in a higher runtime per processor, while the communication overhead remains unaffected. As $\text{T}_{overhead}$ is modelled as a fraction of $\text{T}_{total}$, this lower $\text{T}_{overhead}$ is to be expected.